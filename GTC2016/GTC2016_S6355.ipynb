{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creative Commons CC-BY 4.0, code under MIT license (c) 2016 Pi-Yueh Chuang, L. A. Barba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using AmgX to Accelerate PETSc-Based CFD Codes\n",
    "=================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Pi-Yueh Chuang & Lorena A. Barba**, George Washington University.\n",
    "\n",
    "This technical report supplements the presentation by Pi-Yueh Chuang at the [2016 GPU Technology Conference](http://www.gputechconf.com), on Thu. April 7th: see session [S6355](http://mygtc.gputechconf.com/quicklink/W7yvg9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PetIBM](https://github.com/barbagroup/PetIBM) is a parallel code to study flow around immersed bodies, solving the Navier-Stokes equations on a Cartesian mesh. The body can be embedded in the fluid mesh via the so-called _immersed boundary method_ (IBM), which brings many advantages in regards to mesh generation. The equations are discretized with a finite-difference scheme, via a pressure-projection method, resulting in a Poisson-like linear system. PetIBM uses the [PETSc](https://www.mcs.anl.gov/petsc/) library to solve the linear system of equations in parallel on CPU hardware, and now we want to accelerate it with the help of GPU computing. \n",
    "\n",
    "Our group previously developed another code, cuIBM, which implements the same methods as PetIBM, except that it runs on a sigle CPU and a single GPU, using the [CUSP](http://cusplibrary.github.io) library to solve the linear system. The limited memory on a single GPU restricts the size of CFD problems that we can solve using cuIBM to about 4 million mesh points. In three dimensions, we often need to solve larger problems, which led us to develop PetIBM. Our next step was to add the capability of exploiting multi-GPU systems. \n",
    "\n",
    "In this work, we set out to accelerate PetIBM using the NVIDIA [AmgX](https://developer.nvidia.com/amgx) library on multi-GPUs. We wrote a wrapper code to couple AmgX and PETSc, so that we require few modifications in PetIBM. Our AmgX wrapper is able to handle the situation where the PETSc-based code launches more MPI processes than the number of GPUs available, achieving heterogeneous speed-up. It is open source and we hope others may find it useful to accelerate their PETSc-based parallel programs with multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief overview of PetIBM\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know that solving Poisson problems is computationally demanding. It is particularly challenging in unsteady computational fluid dynamics (CFD), where we need to solve the Poisson system over and over again during a time-marching iteration. The situation in PetIBM is tougher than typical unsteady CFD problems because the coefficient matrix is not a standard discrete Laplace operator: it is augmented by terms derived from the presence of the body. We are in fact solving *modified Poisson equation* at every time step, and this is responsible for about 90% of the run time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particular immersed-boundary formulation in PetIBM is due to Taira and Colonius [1]. The governing equations for the fluid velocity, $\\bf{u}$, and pressure, $p$, are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial \\bf{u}}{\\partial t} + \\bf{u}\\cdot\\nabla\\bf{u} = \n",
    "-\\nabla\\it{p}+\\frac{1}{Re}\\nabla^2\\bf{u}+\\int_s{\\bf{f}(\\bf{\\xi}(\\it{s}, \\it{t}))\\delta(\\bf{\\xi}-\\bf{x})}d\\it{s}, \\\\\n",
    "& \\nabla\\cdot\\bf{u}=0\\it{,} \\\\\n",
    "& \\bf{u}(\\bf{\\xi}(\\it{s}, \\it{t})) = \n",
    "\\int_\\bf{x}{\\bf{u}(\\bf{x})\\delta(\\bf{x}-\\bf{\\xi})}d\\bf{x}=\\bf{u}_\\it{B}(\\bf{\\xi}(\\it{s}, \\it{t}))\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first equation, Navier-Stokes is augmented by a source term that represents the effect of the body on the fluid, via the no-slip boundary condition. Here, $\\bf{f}$ is a singular force distribution along the solid boundary. The second equation enforces the conservation of mass in an incompressible fluid, and the third equation states that the fluid velocity is the same as the body velocity, $\\bf{u}_\\it{B}$ at the boundary (no slip). After discretization, we can write the above equations as a block system of linear equations, in which each submatrix represents an operator or a term in the PDE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "A & G & E^T \\\\\n",
    "G^T & 0 & 0 \\\\\n",
    "E & 0 & 0\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "q^{n+1} \\\\\n",
    "\\phi \\\\\n",
    "\\tilde{f}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "=\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "r^n \\\\\n",
    "0 \\\\\n",
    "u_B^{n+1}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "+\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "bc_1 \\\\\n",
    "-bc_2 \\\\\n",
    "0\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $q^{n+1}$ are the momentum fluxes at cell boundaries on the next time step, $n+1$, while $r^n$ are the convective terms on the previous time step; $\\phi$ is a vector of pressure values and $\\tilde{f}$ is a vector of force values at the boundary points. We can further define some larger submatrices and perform a block-LU decomposition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q\\equiv\\left[G, E^T\\right],\\ \n",
    "\\lambda\\equiv\\left(\\begin{matrix}\\phi \\\\ \\tilde{f}\\end{matrix}\\right),\\ \n",
    "r_1\\equiv r^n + bc_1,\\ \n",
    "r_2\\equiv\\left(\\begin{matrix}-bc_2 \\\\ u_B^{n+1}\\end{matrix}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "A & 0 \\\\\n",
    "Q^T & -Q^TB^NQ\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "I & B^NQ \\\\\n",
    "0 & I\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "q^{n+1} \\\\\n",
    "\\lambda\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "=\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "r_1 \\\\\n",
    "r_2\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "+\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "-\\frac{\\Delta t^N}{2^N}\\left(LM^{-1}\\right)Q\\lambda \\\\\n",
    "0\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix system corresponds to solving three algebraic equations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& Aq^*=r_1 \\\\\n",
    "& Q^TB^NQ\\lambda=Q^Tq^*-r_2 \\\\\n",
    "& q^{n+1}=q^*-B^NQ\\lambda\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $q^{*}$ is the intermediate solution for the fluxes, used in the pressure projection step. The second equation is a *modified Poisson equation*, corresponding to a Poisson equation plus terms that are due to the presence of the body. The coefficient matrix in the modified Poisson eqauation, $Q^TB^NQ$, has a non-zero pattern like that shown in Figure 1, which corresponds to the situation where a circular cylinder is immersed in a fluid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/cylinder3dRe40QTBNQ.png\" style=\"width:300px;height:300px;\" />\n",
    "\n",
    "##### Figure 1. Non-zero pattern of the modified Poisson coefficient matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper-left block in the coefficient matrix corresponds to the typical Poisson system, i.e., the discrete Laplace operator. The bottom, bottom-right corner and the right side of the matrix in Figure 1 are the modification terms due to the presence of the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PetIBM, solving this modified Poisson equation takes over **90%** of the total run time. We will introduce GPU computing to accelerate only this part of the overall calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AmgX\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AmgX](https://developer.nvidia.com/amgx) is a library that provides linear solvers and preconditioners with capability of using multiple GPUs. It is developed and supported by NVIDIA, having entered beta production in January 2014. A tutorial, user's manual and license information are available on the [NVIDIA Developer website](https://developer.nvidia.com/amgx).\n",
    "\n",
    "Here we list some of the features of AmgX:\n",
    "\n",
    "* Krylov methods:\n",
    "    * CG, GMRES, BiCGStab, ... etc\n",
    "* Multigrid (MG) preconditioners:\n",
    "    * Classical algebraic multigrid (AMG)\n",
    "    * Unsmoothed aggregation AMG\n",
    "* Smoothers:\n",
    "    * Block-Jacobi, Gauss-Seidel, incomplete LU, dense LU, ... etc\n",
    "* Cycles:\n",
    "    * V, W, F, FG, CGF\n",
    "* Multiple GPUs on single node / multiple nodes\n",
    "    * MPI (OpenMPI) / MPI Direct\n",
    "    * Single MPI rank <=> single GPU\n",
    "    * Multiple MPI ranks <=> single GPU\n",
    "* C API\n",
    "* Unified Virtual Addressing\n",
    "\n",
    "The various combinations of Krylov methods and multigrid preconditioners are very popular in CFD applications. In fact, the AmgX library was developed specifically aimed for CFD and similar computational physics applications, in a partnership between NVIDIA and ANSYS, a major provider of commercial CFD software.\n",
    "\n",
    "[Benchmarks](https://devblogs.nvidia.com/parallelforall/amgx-v1-0-enabling-reservoir-simulation-with-classical-amg/) published by NVIDIA in the Developer Blog (May 15, 2014) compared against the \"gold standard\" classical AMG implementation: BoomerAMG in the HYPRE package. They showed speedups of 2x in the setup phase and 4–6x in the solve phase, when comparing a single K40 GPU versus 2x10-core Intel Ivy Bridge CPUs, on problems of size 5 million and 10 million. But results on multi-GPUs were still unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our AmgX wrapper\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we need to write an AmgX \"wrapper\" code?\n",
    "\n",
    "AmgX requires the application code to provide it with the coefficient matrix, and the right-hand-side vector of the linear system. The application can create these data structures in the host CPU (then copy them to the device), or in-place in the GPU.\n",
    "\n",
    "But our application code uses the PETSc library. Both AmgX and PETSc have their own data structures for parallel linear algebra. PetIBM is also written in C++ with some object-oriented design, while AmgX offers a pure-C API. Thus, to use AmgX directly in PetIBM, we would need to add or modify a lot of code, which we would rather avoid. \n",
    "\n",
    "Also, we may later on want to develop some other software using PETSc, and we may also want to use AmgX with it. So a wrapper to couple AmgX and PETSc could help us or others in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The philosophy of our wrapper is simple: except when initializing and finalizing, users should only need to call `setA` and `solve` in most cases. In unsteady CFD applications, with time-marching iterations, users would simply call `solve` repeatedly.  But there should be no need to be concerned about data conversion: `A`, `x`, and `rhs` are all PETSc data structures, and remain so, on the application side. We also would like configuration to be done simply through an input script during runtime.\n",
    "\n",
    "The wrapper code equips the PETSc application software with access to AmgX, without the burden of unpacking data structures from one into the other. Without it, to use AmgX from PETSc one would have to access the raw data from the sparse matrix in PETSc, then redistribute as needed across MPI processes to consolidate the data in a way that can be sent to the GPU device. Next, the application would need to initialize AmgX and allocate memory on the GPU—using the  API only, users would need about 10 different AmgX functions for just this part. Users would need more code to set up the matrix and vectors and send from host to device, then set up the solver, solve and finalize.  With our wrapper, users only need to declare an instance of the solver with `AmgXWrapper solver` and then call `solver.initialize(...)`. The diagram in Figure 2 shows the simple work flow provided by our wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/wrapper.png\" style=\"width:500px;height:300px;\" />\n",
    "\n",
    "##### Figure 2. Diagram of our AmgX wrapper work flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical tests\n",
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Test: 2D Cylinder Flow, Re=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the wrapper is just a wrapper. It's just an interface between PETSc ans AmgX and should work well. Let's see how it works. First we run tests with a 2D cylinder flow (Re=40). Mesh size is 2.25M.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/rawAmgXMultiRanks_Cylinder2d_2.png\" style=\"width:500px;height:500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The far left bar is the result from original CPU-version PetIBM with 6 CPU cores. We can see that solving modified Poisson equation, which is the blue portion, takes over 90% of total time in this case. \n",
    "\n",
    "The middle bar is the result from using the AmgX-version PetIBM with 1 GPU and 1 CPU core. We get 4 times speed-up on solving modified Poisson equation and 3 times speed-up for total time. We think it looks great, but not great enough. From the parts that are solved on CPUs, which are the portions with colors other than blue, we find now it takes more time to solve them. The reason is obvious: in this case we only use 1 CPU core to solve these CPU-based parts, while 6 CPU cores were used to solve them in the far left bar. Let's see how if we use 6 CPU cores to solve the CPU-based parts and 1 GPU to solve the modified Poisson system.\n",
    "\n",
    "The far right bar is the result. We indeed get the same performances on the parts solved or calculated with CPUs, but the speed-up of solving modified Poisson equation is not good as we expected. Actually, it's totally unacceptable for me. So, what happened?\n",
    "\n",
    "It's not difficult to answer it. Just think about what will happen if you solve a linear system with domain-decomposition methods and partition the system into 6 subdomains but solve them with only one CPU core or less than 6 CPU cores. Situation here is the same.\n",
    "\n",
    "Recall that AmgX solvers only solve the linear system. That means we need to prepare the linear systems on CPUs and using PETSc. So when we launch PetIBM with 6 CPU cores, the modified Poisson system is partitioned into 6 subdomains through some partitioning methods in PETSc. And when we call AmgX to solve the modified Poisson system, each CPU core creates an AmgX solver to solve the subdomain it holds. In our case, there are 6 subdomain-solvers but only one GPU. This means the 6 subdomain-solvers are competing GPU resources. And remember we are using domain-decomposition methods, so information exchanges occurs between these 6 subdomain-solvers and on one single GPU. Solving modified Poisson equation in the far right bar therefore is not as fast as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More examples revealing this issue:\n",
    "\n",
    "Let's see more examples revealing this issue:\n",
    "\n",
    "#### 1. Solving 3D Poisson equations (6M unknowns; CG solver; Classical AMG preconditioner):\n",
    "\n",
    "<img src=\"plots/rawAmgXMultiRanks_Classical.png\" style=\"width:450px;height:300px;\" />\n",
    "\n",
    "\n",
    "#### 2. Solving modified Poisson equations from cylinder flow simulations (6M unknowns; CG solver; Aggregation AMG preconditioner)\n",
    "\n",
    "<img src=\"plots/rawAmgXMultiRanks_Aggregation.png\" style=\"width:450px;height:300px;\" />\n",
    "\n",
    "Both cases show that when we prepare linear systems with PETSc and more CPU processes than the number of GPUs we used, the performances of GPU solvers get worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "--------------\n",
    "\n",
    "Solution is not complicated. We just need to prepare the linear systems that AmgX is going to solve with the same number of subdomains as that of GPUs. The purpose is to have only one subdomain-solver on each GPU.\n",
    "\n",
    "If we are going to modified the source code of PetIBM, there are plenty of ways to reach the goal. No matter using which approach, we can not avoid doing a lot modifications, because while we want to use only the same number of CPU processes as that of GPUs to handle the modified Poisson equation, we still want to use as many CPU processes as possible to calculate and solve the parts other than modified Poisson equation, like the velocity system, for example.\n",
    "\n",
    "So, given that the reasons why we write a wrapper are that we want to make using AmgX easy and that we want to do as few modifications as possible in PetIBM, we should implement the solution to this issue into the wrapper, instead of directly modify the source code of PetIBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is demonstrated in this figure:\n",
    "\n",
    "<img src=\"plots/mergeSysOnHosts.png\" style=\"width:450px;height:300px;\" />\n",
    "\n",
    "The implementation is achieved through communicator splitting. We briefly illustrate this step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Split the global communicator (eg. MPI_COMM_WORLD) into communicators representing each node. \n",
    "<table border=\"0\" style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td style=\"border-style: hidden; border-collapse: collapse;\"><img src=\"plots/commSplit1.png\" style=\"width:400px;height:200px;\" /></td>\n",
    "    <td style=\"border-style: hidden; border-collapse: collapse;\">$\\Longrightarrow$</td> \n",
    "    <td style=\"border-style: hidden; border-collapse: collapse;\"><img src=\"plots/commSplit2.png\" style=\"width:400px;height:200px;\" /></td>\n",
    "  </tr>\n",
    "</table>\n",
    "When we launch a program, we know how many CPU processes, computing node, and the number of GPUs on each node, but the program doesn't. Without additional coding, the program only knows the total number of CPU processes. So the first step is to split the global communicator, which is typically `MPI_COMM_WORLD` into communicators representing each computing node. Then we can calculate how many GPU devices there are on each node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Devide the CPU processes equaly according to the number of GPUs on each node\n",
    "<img src=\"plots/commSplit3.png\" style=\"width:400px;height:200px;\" />\n",
    "So that for each node, the number of CPU processes sharing one GPU is the same with those sharing another GPU on the same node. This is also done with communicator splitting. Each ***matrix-consolidation communicator*** represents CPU processes that share the same GPU. And their data, or says, the subdomains they hold, will be consolidated and gatherred before being transferred to GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Assign one CPU process in each matrix-consolidation communicator to be in charge of launching AmgX subdomain-solver\n",
    "<img src=\"plots/commSplit4.png\" style=\"width:400px;height:200px;\" />\n",
    "Normally, they are rank zero in each matrix-consolidation communicator. Only these CPU processes can talk to GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Create a new communicator that holds all CPU processes that can talk to GPUs\n",
    "<img src=\"plots/commSplit5.png\" style=\"width:400px;height:200px;\" />\n",
    "Because we are solving linear equations using domain-decomposition methods, AmgX subdomain-solvers on GPUs need to talk to each others. Since GPUs can not talk to each others directly, their communications are done through CPUs and MPI. So we need to create a communicator for the CPUs that can talk to GPUs, and then they can talk to each others, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only consolidation of the coefficient matrix is handled in the wrapper, the gathering and scattering of unknown and right-han-side vectors are also handled in the wrapper, of course.\n",
    "\n",
    "We implement this in the wrapper, and this keeps the usage of AmgX and the wrapper simple in PetIBM or other PETSc-based applications. We still only need to call `setA` and `solve` and don't need to worry about other else. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether this is worth or not\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my previous brief illustration, you can find there are some extra communications we need in order to do matrix consolidation or gather and scatter vectors. Communications take time. So we need to do some tests to see whether this is worth or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Back to the 3D Poisson system\n",
    "\n",
    "<img src=\"plots/newAmgXMultiRanks_Classical.png\" style=\"width:450px;height:300px;\" />\n",
    "\n",
    "#### 2. Back to the modofied Poisson systems\n",
    "\n",
    "<img src=\"plots/newAmgXMultiRanks_Aggregation.png\" style=\"width:450px;height:300px;\" />\n",
    "\n",
    "We can see now no matter how many CPU processes we used to prepare the linear system, the times used to solve the system on GPU with AmgX and wrapper are almost the same.\n",
    "\n",
    "#### 3. Back to the 2D cylinder flow\n",
    "\n",
    "<img src=\"plots/newAmgXMultiRanks_Cylinder2d_2.png\" style=\"width:500px;height:500px;\" />\n",
    "\n",
    "The far right bar now is the result from PetIBM with AmgX and host-side consolidation enabled in the wrapper. Now the time used to solve the modified Poisson equation, which is the blue portion, is the same as the middle bar. And we can see, from the portions with colors other than blue, now we can use as many CPU cores as possible. This means, with our wrapper, users can utilize all CPU and GPU resources they have to achieve the best performance by just calling `setA` and `solve` in their existing PETSc-based programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests and benchmarks\n",
    "---------------------------------\n",
    "\n",
    "Since our wrapper is ready, we can do some tests and benchmarks to see how the performance of AmgX is. As I said, our goal here is to accelerate our CFD programs, PetIBM, so our tests focus on two problems. The first one is Poisson equations, which represents the flows without solid body present, like lid-driven flows. The second one is modified Poisson equation. For this one, we will directly use PetIBM to run a real application and see how much speed-up we can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Poisson Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Small-size problem\n",
    "\n",
    "<table border=\"0\" style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td style=\"border-style: hidden; border-collapse: collapse;\"><img src=\"plots/Poisson_N1M2D.png\" style=\"width:450px;height:275px;\" /></td>\n",
    "    <td style=\"border-style: hidden; border-collapse: collapse;\"><img src=\"plots/Poisson_N1M3D.png\" style=\"width:450px;height:275px;\" /></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Well, it seems AmgX doesn't benefit a lot in both 2D and 3D problems.\n",
    "\n",
    "In CPU parts, blue bars represent the results of using PETSc KSP solvers and PETSc gamg preconditioners, while purple bars are those using Hypre preconditioners but still with PETSc KSP solvers. In GPU parts, red bars are the results from GPU clusters in HPC center of our university, and the green bars are the results from a workstation in our lab. By having both results from GPU clusters and GPU workstation, we want to see if it is possible that, when we have GPU computing, even a workstation can be competitive to traditional CPU clusters.\n",
    "\n",
    "Since the result in small-size problem seems not as good as we expected, we want to see whether this is because the problem size is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Medium-size problem\n",
    "\n",
    "<table border=\"0\" style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td style=\"border-style: hidden; border-collapse: collapse;\"><img src=\"plots/Poisson_N25M2D.png\" style=\"width:450px;height:275px;\" /></td>\n",
    "    <td style=\"border-style: hidden; border-collapse: collapse;\"><img src=\"plots/Poisson_N25M3D.png\" style=\"width:450px;height:275px;\" /></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Now we have 25 millions unknowns in our problem, which might not be unusual in aerodynamics or hydrodynamics industry. \n",
    "\n",
    "In CPU parts, we take off the results from GAMG preconditioners, because it is much slower than Hypre in this case. And in GPU parts, we need at least 4 and 8 K20 GPU in 2D and 3D problems respectively because of limited memory on single K20. Also, since we only have 2 K40 on our workstation, so the workstation can not handle the 3D case.\n",
    "\n",
    "I don't know what you think, but for me, I think the result is not bad.\n",
    "\n",
    "In the 2D case, since the CPU results seem to have good scalability, we can predict that, if we want to achieve 5 times speed-up, we may need probably 100 CPU cores. But if we use AmgX, we only need 4 K20. I will say four K20 is better than 100 CPU cores. Because we can actually have 4 K20 on our personal workstation, an no need to use clusters. Also, don't forget K20 is kind of an old model. You can probably find it on Amazon at a price of 25-hundred dollars for a new one. So, 4 K20 may cost 10 thousands, while I don't think 10 thousand can give you a CPU cluster with 100 CPU cores.\n",
    "\n",
    "And in 3D case, again, if we want to achieve 13.6 times speed-up by only CPU computing, we may need 256 CPU cores, while we only need 8 K20 when we use AmgX. I'll say 8 K20 is better than 256 CPU cores. The same reason as I just illustrated in the 2D case. Though it may not be possible to hold 8 K20 in a personal workstation, but it is possible to install 8 K20 in an existing node in HPC clusters. Also, K20 is an old model, if we use K40 or K80, we can probably still use our personal workstation to compete against 256-CPU-core cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Large-size problem\n",
    "\n",
    "<table border=\"0\" style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td style=\"border-style: hidden; border-collapse: collapse;\"><img src=\"plots/Poisson_N100M2D.png\" style=\"width:450px;height:275px;\" /></td>\n",
    "    <td style=\"border-style: hidden; border-collapse: collapse;\"><img src=\"plots/Poisson_N50M3D.png\" style=\"width:450px;height:275px;\" /></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Though results of medium-size Poisson problem are good, I want to show you the results from large-size problem, just in case some of you are interested in simulations in this scale.\n",
    "\n",
    "For the 2D case, in which we have 100 millions unknowns, we can predict that we may need almost 400 CPU cores to achieve the same speed-up as 32 GPUs. For the 3D case, we can only do 50 millions unknowns because the limited memory we have on all our GPUs. We may also need approximate 400 CPU cores to achieve the same speed-up.\n",
    "\n",
    "And again, K20 is a very new model. So if we can have newer model, like K80, which has larger memory, we may need only few of them to achieve the same performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Flying Snake\n",
    "\n",
    "For PetIBM, Poisson equation represents flows without solid body present, which is normally not real cases we simulate with. PetIBM is for flows passing some bodies, so solving modified Poisson equation is more common. Here, we use a real application we did with PetIBM as an example to demonstrate how AmgX can accelerate solving modified Poisson equation and hence accelerate the whole simulation.\n",
    "\n",
    "Simulations of flying snakes are done by one of our group member, Anush. You can find details of flying snake in his 2014 paper[3].\n",
    "\n",
    "<img src=\"plots/flyingSnake_Vorticity2.png\" style=\"width:450px;height:275px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-run one of the flying snake simulations with CPU cluster, K20 GPU cluster, and a personal workstation with 2 K40 GPU. Here are short specs of the hardware:\n",
    "\n",
    "1. 1 node in CPU cluster = 12 CPU cores (2 Intel E5-2620)\n",
    "2. 1 node in GPU cluster = 1 CPU node (i.e. 12 CPU cores) + 2 K20 GPU\n",
    "3. personal workstation = 6 CPU cores (1 Intel i7-5930K) + 2 K40c GPU\n",
    "\n",
    "You can find computing nodes in the GPU cluster are actually nodes in the CPU cluster with additional two K20 installed on each node. This setting can help us to determine whether using AmgX and GPU computing is beneficial from the viewpoint of money cost on hardware.\n",
    "\n",
    "<img src=\"plots/flyingSnake.png\" style=\"width:800px;height:600px;\" />\n",
    "\n",
    "If we use the result of 1 CPU node as our baseline, then the speed-up of using 1 GPU node is almost 21 times. Again, based on the scalability of CPU results, we can predict that we may need 20 CPU nodes in order to achieve 21 times speed-up if we are using pure CPU code. Recall one GPU node is actually 1 CPU node plus 2 K20. I think it's obvious that adding 2 K20 to an existing computing node is cheaper than adding 19 extra computing nodes.\n",
    "\n",
    "Next let's move on to the results of using the workstation. I think the result here is much more exciting. When we use our personal workstation and one K40 GPU, we can get about 15 times speed-up, which can also be achieved with 16 CPU nodes in the CPU cluster. This means, at least for our applications, if the GPU memory on our workstation is large enough, we don't need to run our simulations on clusters. Our personal workstation can handle it.\n",
    "\n",
    "So from these results, the benefit of AmgX and GPU computing for our CFD code, PetIBM, is very obvious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational cost: time is money\n",
    "---------------------------------------------\n",
    "\n",
    "So far we have mostly focused on the saving of time that AmgX can benefits us. But is that all? What else can those benchmarks imply?\n",
    "\n",
    "We all know time is money. So when AmgX saves time for us, it also saves money for us. I want to discuss a little bit more here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Potential saving on hardware\n",
    "\n",
    "I mentioned a little bit about this when I showed the results to you just now. For example, from the flying snake simulation, using one workstation and one K40 GPU is competitive with a CPU cluster of 16 nodes. And of course using workstation can save you a lot. If you want to build a HPC cluster, you will need to consider costs on hardware like motherboards, memory, hard drives, cooling systems, power supplies, network switches, physical spaces ... etc. But now we only need to buy two GPUs and install them onto PCI-E slots of an existing workstation. \n",
    "\n",
    "Furthermore, if we can avoid building clusters, it does not only reduce the cost on hardware, but also reduce the cost on human resources, because you probably won't need to hire an IT guy to manage your cluster for you, or you won't need to add more work loading to your group members.\n",
    "\n",
    "Image that when you want to promote your CFD software to customers who have limited budget on hardware but want to get\n",
    "results quickly because of rapid developing cycle, would you prefer recommending them to build a cluster or recommending them to buy several GPUs and install them on existing workstation?\n",
    "\n",
    "Of course we may still need clusters sometimes because of limited memory on GPUs and limited PCI-E slots on a single motherboard. But from our benchmark results, we can see that, with AmgX, even when we need clusters, we need much fewer computing nodes. This implies a lot, such as: less socket-to-socket communication; reducing potential runtime crash due to single-node failure or network failiure; or you can get higher priority in queue if you run simulations in HPC centers, because now you need less time and less nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Potential saving on cloud HPC service\n",
    "\n",
    "I also want to discuss about the benefit AmgX and GPU computing can give us when we use cloud HPC service, such as Amazon EC2.\n",
    "\n",
    "We run benchmarks on Amazon EC2 to see what we can get. I list the specs of GPU nodes and CPU nodes we used on Amazon EC2 here:\n",
    "\n",
    "<table border=\"0\" width=100%>\n",
    "    <tr>\n",
    "        <td style=\"border-style: hidden; border-collapse: collapse;\" width=50%>\n",
    "            <ul> \n",
    "                <li> \n",
    "                    GPU nodes - g2.8xlarge:  \n",
    "                    <ul>\n",
    "                        <li> 32 vCPU (Intel E5-2670) + 4 GPUs (Kepler GK104 ) </li>\n",
    "                        <li> Official Price: &#36;2.6 / hr </li>\n",
    "                        <li> Possible Lower Price (Spot Instances): < &#36;0.75 / hr </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </td>\n",
    "        <td style=\"border-style: hidden; border-collapse: collapse;\" width=50%>\n",
    "            <ul> \n",
    "                <li> \n",
    "                    CPU nodes - c4.8xlarge\n",
    "                    <ul>\n",
    "                        <li> 36 vCPU (Intel E5-2663) </li>\n",
    "                        <li> Official Price: &#36;1.675 / hr </li>\n",
    "                        <li> Possible Lower Price (Spot Instances): < &#36;0.6 / hr </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "\n",
    "        </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results:\n",
    "\n",
    "<img src=\"plots/Amazon.png\" style=\"width:500px;height:450px;\\\" />\n",
    "\n",
    "Even we use 8 CPU nodes, we can still get 3 times speed-up on 1 GPU node! It's kind of surprising but not so surprising. The reason why it is surprising is because 8 CPU nodes have a total of 288 CPU cores! Well, but it is actually not so surprising when we consider that the nodes are virtual machines, not physical machines. We don't know what's inside Amazon's black box.\n",
    "\n",
    "But one thing we can know is how much we can save!  First let's calculate the actual fee we paid for the simulations. We use spot instances, so the real fees we paid for nodes are:\n",
    "\n",
    "* CPU nodes:\n",
    "$$12.5hr \\times \\$0.5/hr \\times 8nodes={\\underline {\\bf \\$50.0}}$$\n",
    "* GPU nodes:\n",
    "$$4hr \\times \\$0.5/hr \\times 1nodes={\\underline {\\bf \\$2.0}}$$\n",
    "\n",
    "Wow, we save 25 times on the cost!\n",
    "\n",
    "Well, sometimes it's difficult to get spot instances or we are not willing to use them. Then, how much we can save if we rent the instances with official price? \n",
    "\n",
    "* CPU nodes:\n",
    "$$12.5hr \\times \\$1.675/hr \\times 8nodes={\\underline {\\bf \\$167.5}}$$\n",
    "* GPU nodes:\n",
    "$$4hr \\times \\$2.6/hr \\times 1nodes={\\underline {\\bf \\$10.4}}$$\n",
    "\n",
    "We still get about 16 times saving!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "-------------------\n",
    "\n",
    "To conclude, we use AmgX to accelerate our CFD code, PetIBM. And we also write a wrapper to couple AmgX and PETSc for the purpose of easily using AmgX. Our wrapper can handle the situation that we want to launch the PETSc-based program with more MPI processes than the number of GPU we have. So with wrapper, we can utilize as many CPU processes as possible to run the parts still remain on CPU, and use as many GPU as possible to accelerate solving tough parts.\n",
    "\n",
    "You can download AmgX, our wrapper, and PetIBM with multi-GPU computing enabled from:\n",
    "\n",
    "* AmgX: https://developer.nvidia.com/amgx\n",
    "* AmgX Wrapper: https://github.com/barbagroup/AmgXWrapper\n",
    "* PetIBM: https://github.com/barbagroup/PetIBM/tree/AmgXSolvers\n",
    "\n",
    "We also do some benchmarks and see how AmgX help us to save not only time but also money costs, no matter from the viewpoints of hardware or cloud HPC service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "----------------------------\n",
    "\n",
    "I would like to give very special thanks to Dr. Joe Eaton from NVIDIA. He gave us a lot of help on this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "-----------------\n",
    "\n",
    "[1] K.  Taira and T.  Colonius, \"The immersed boundary method: A projection approach\", Journal of Computational Physics, vol. 225, no. 2, pp. 2118-2137, 2007\n",
    "\n",
    "[2] A.  Krishnan, J.  Socha, P.  Vlachos and L.  Barba, \"Lift and wakes of flying snakes\", Physics of Fluids, vol. 26, no. 3, p. 031901, 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "<link href='https://fonts.googleapis.com/css?family=Source+Code+Pro' rel='stylesheet' type='text/css'>\n",
       "<link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300italic,300,200italic,200,400italic,600,600italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 850px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running { \n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256); \n",
       "    border-radius: 0px; \n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Source Sans Pro' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 130%;\n",
       "    font-weight: 300;\n",
       "    width: 700px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Roboto Slab', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;    \n",
       "    font-size: 35pt;\n",
       "    line-height: 110%;\n",
       "    color: rgb(17,102,172);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 1.0em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Roboto Slab', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 24pt;\n",
       "    line-height: 110%;\n",
       "    color: rgb(102,172,17);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\t\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Roboto Slab', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 20pt;\n",
       "    font-weight: 300;\n",
       "    line-height: 110%;\n",
       "    margin-bottom: 3px;\n",
       "    color: rgb(172,164,17);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for small titles*/\n",
       "    font-family: 'Roboto Slab', sans-serif;\n",
       "    font-weight: 200;\n",
       "    font-size: 14pt;\n",
       "    line-height: 110%;\n",
       "    color: rgb(172,87,17);\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 1em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for captions*/\n",
       "    font-family: 'Roboto Slab', serif;\n",
       "    font-size: 14pt;\n",
       "    font-weight: 200;\n",
       "    font-style: italic;\n",
       "    text-align: center;\n",
       "    margin-top: 3em;\n",
       "    margin-bottom: 2em;\n",
       "\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"Source Code Pro\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".alert-box {\n",
       "    padding:10px 10px 10px 36px;\n",
       "    margin:5px;\n",
       "}\n",
       "\n",
       ".success {\n",
       "    color:#666600;\n",
       "    background:rgb(240,242,229);\n",
       "}\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../styles/GTC16style.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
